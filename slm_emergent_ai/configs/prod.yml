"""
本番環境設定ファイル - 実運用用設定

本番環境での実行用の最適化された設定ファイル。
パフォーマンスとメモリ使用量のバランスを考慮した設定。
"""

# SLM Emergent AI 本番環境設定
model:
  path: "gemma3:1b"  # モデルパス
  quantize: "q4"     # 量子化レベル (最小メモリ使用量)

agent:
  n: 10              # エージェント数
  max_turns: 20      # 最大ターン数
  roles:             # エージェントの役割
    - "質問者"
    - "回答者"
    - "批評者"
    - "要約者"
    - "検証者"
    - "創造者"
    - "分析者"
    - "調停者"
    - "探索者"
    - "統合者"

lambda:
  a: 0.35            # 整列ルールの重み
  c: 0.35            # 結合ルールの重み
  s: 0.08            # 分離ルールの重み (安定性向上のため低め)

memory:
  mode: "distributed" # メモリモード (分散モード)
  redis_url: "redis://redis:6379/0"  # Redis URL

controller:
  target_H: 5.2      # 目標エントロピー
  Kp: 0.12           # 比例ゲイン
  Ki: 0.015          # 積分ゲイン
  Kd: 0.06           # 微分ゲイン

metrics:
  port: 7861         # メトリクスサーバーポート
  log_interval: 5    # ログ間隔 (秒)
  history_size: 3600 # 履歴サイズ (1時間分)

ui:
  enabled: true      # UIの有効化
  port: 7860         # UIポート
  update_interval: 2 # 更新間隔 (秒)

rag:
  enabled: true      # RAGの有効化
  backend: "elastic" # バックエンド (elastic)
  config:
    es_url: "http://elasticsearch:9200"
    index_name: "slm_rag_prod"

runtime:
  threads: 12        # スレッド数
  batch_size: 4      # バッチサイズ
  kv_cache: true     # KVキャッシュの有効化
  prefill_batch: true # Prefillバッチ化

logging:
  level: "INFO"      # ログレベル
  file: "prod.log"   # ログファイル
  rotation: "1d"     # ログローテーション

monitoring:
  prometheus: true   # Prometheusの有効化
  port: 9090         # Prometheusポート
  alert_thresholds:  # アラート閾値
    memory_percent: 90
    cpu_percent: 95
    tok_s_drop: 50

prompts:
  initial:           # 初期プロンプト
    - "こんにちは、私は質問者です。今日のトピックについて議論しましょう。"
    - "こんにちは、私は回答者です。どのようなご質問でもお答えします。"
    - "こんにちは、私は批評者です。議論の質を高めるためにコメントします。"
    - "こんにちは、私は要約者です。議論の要点をまとめます。"
    - "こんにちは、私は検証者です。情報の正確性を確認します。"
    - "こんにちは、私は創造者です。新しいアイデアを提案します。"
    - "こんにちは、私は分析者です。議論を深く分析します。"
    - "こんにちは、私は調停者です。議論の調和を保ちます。"
    - "こんにちは、私は探索者です。新しい視点を探ります。"
    - "こんにちは、私は統合者です。様々な意見を統合します。"
